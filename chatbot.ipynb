{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"chatbot.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN3Jj3ygj9IbyL/csQYNewa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Z9gwB1o7Wgxd","colab_type":"text"},"source":["--- \n","--- \n","# Chatbot\n","--- \n","--- "]},{"cell_type":"markdown","metadata":{"id":"BGL0MmIYecO2","colab_type":"text"},"source":["Ce notebook regroupe les différents modèles créés sur les autres notebook afin de faire fonctionner notre chatbot."]},{"cell_type":"markdown","metadata":{"id":"LD3pysYSWvrz","colab_type":"text"},"source":["---\n","## Importation des bibliothèques et modules\n","---"]},{"cell_type":"code","metadata":{"id":"T1bP0BHq1K1W","colab_type":"code","outputId":"5d1b6181-2ccf-419d-b261-a16877592a08","executionInfo":{"status":"ok","timestamp":1580663739988,"user_tz":-60,"elapsed":2787,"user":{"displayName":"sarah faucon","photoUrl":"","userId":"00071383878885981401"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import keras\n","import tensorflow as tf \n","import numpy\n","import pandas as pd\n","import random\n","import re\n","import numpy as np\n","import csv\n","import pickle\n","import spacy\n","import nltk\n","from nltk.stem import SnowballStemmer\n","from nltk.tokenize import word_tokenize"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"NA7ocdYp7VrG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":644},"outputId":"d755c795-fa03-49e3-efd4-d980f06e7acd","executionInfo":{"status":"ok","timestamp":1580663744086,"user_tz":-60,"elapsed":6788,"user":{"displayName":"sarah faucon","photoUrl":"","userId":"00071383878885981401"}}},"source":["!pip install --upgrade tensorflow"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.1.0)\n","Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n","Requirement already satisfied, skipping upgrade: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.0)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n","Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.0)\n","Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n","Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n","Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n","Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.5)\n","Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n","Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n","Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n","Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n","Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n","Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (42.0.2)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.16.0)\n","Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n","Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n","Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.11.0)\n","Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n","Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n","Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n","Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n","Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0.0)\n","Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.7)\n","Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n","Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n","Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UasJh6oi7_xb","colab_type":"code","outputId":"6260fbbe-4a49-41bd-bbf3-21dd34d386a3","executionInfo":{"status":"ok","timestamp":1580663744089,"user_tz":-60,"elapsed":6761,"user":{"displayName":"sarah faucon","photoUrl":"","userId":"00071383878885981401"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"98kmcfL2JQkx","colab_type":"text"},"source":["---\n","## Elements du modèle génératif\n","---\n"]},{"cell_type":"code","metadata":{"id":"0nJCPiIG7w2l","colab_type":"code","outputId":"f63bd617-7e0d-4912-c7f4-81519f72a74c","executionInfo":{"status":"ok","timestamp":1580663745155,"user_tz":-60,"elapsed":7794,"user":{"displayName":"sarah faucon","photoUrl":"","userId":"00071383878885981401"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["model =  tf.keras.models.load_model('/content/gdrive/My Drive/generative_model.h5')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kfVOXhAVFNMs","colab_type":"code","colab":{}},"source":["char2idx = pickle.load(open('/content/gdrive/My Drive/lst_char.pkl', \"rb\"))\n","idx2char = pickle.load(open('/content/gdrive/My Drive/lst_idx.pkl', \"rb\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_UailFBbFa76","colab_type":"code","colab":{}},"source":["def generate_text(model, start_string):\n","  # Evaluation step (generating text using the learned model)\n","\n","  # Number of characters to generate\n","  num_generate = 100\n","\n","  # Converting our start string to numbers (vectorizing)\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # Empty string to store our results\n","  text_generated = []\n","\n","  # Low temperatures results in more predictable text.\n","  # Higher temperatures results in more surprising text.\n","  # Experiment to find the best setting.\n","  temperature = 1.0\n","\n","  # Here batch size == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval)\n","      # remove the batch dimension\n","      predictions = tf.squeeze(predictions, 0)\n","\n","      # using a categorical distribution to predict the word returned by the model\n","      predictions = predictions / temperature\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","      # We pass the predicted word as the next input to the model\n","      # along with the previous hidden state\n","      input_eval = tf.expand_dims([predicted_id], 0)\n","\n","      text_generated.append(idx2char[predicted_id])\n","\n","  return (start_string + ''.join(text_generated))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cxtagaOmWUVx","colab_type":"text"},"source":["---\n","## Elements du modèle de classification métier\n","---\n"]},{"cell_type":"code","metadata":{"id":"Q3QMZwsz1XC2","colab_type":"code","colab":{}},"source":["scrapping = pd.read_csv(\"/content/gdrive/My Drive/scraping.csv\", sep=';', header=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fn0mMiuLYVZ1","colab_type":"code","colab":{}},"source":["vectorizer_bin = pickle.load(open('/content/gdrive/My Drive/vectorizer_bin.pkl', \"rb\"))\n","model_bin =  tf.keras.models.load_model('/content/gdrive/My Drive/model_bin.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ogRyXb8louq9","colab_type":"code","colab":{}},"source":["vectorizer_multi = pickle.load(open('/content/gdrive/My Drive/vectorizer_multi.pkl', \"rb\"))\n","classifier_multilog = pickle.load(open('/content/gdrive/My Drive/model_multilog.pkl', \"rb\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXhp3rl73Co0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"d8426e09-2a6c-4e52-82b7-af5c030dd169","executionInfo":{"status":"ok","timestamp":1580663745419,"user_tz":-60,"elapsed":7919,"user":{"displayName":"sarah faucon","photoUrl":"","userId":"00071383878885981401"}}},"source":["nltk.download('stopwords')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"SPcDqUuFTJEV","colab_type":"code","outputId":"6c3c4647-a402-4402-95db-9cea2f2eaaf3","executionInfo":{"status":"ok","timestamp":1580663748252,"user_tz":-60,"elapsed":10737,"user":{"displayName":"sarah faucon","photoUrl":"","userId":"00071383878885981401"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"source":["!python -m spacy download fr_core_news_sm"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: fr_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-2.1.0/fr_core_news_sm-2.1.0.tar.gz#egg=fr_core_news_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('fr_core_news_sm')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"da7QG16S21Hs","colab_type":"code","colab":{}},"source":["stemmer = SnowballStemmer('french')\n","sw = nltk.corpus.stopwords.words(\"french\")\n","sw.extend([\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"])\n","sw.extend([\"dans\",\"si\",\"même\",\"mêmes\",\"tous\",\"tout\",\"toutes\",\"puis\",\"depuis\",\"lors\",\"votre\",\"notre\",\"ainsi\",\"ici\",\n","          \"prendre\",\"avoir\",\"rendre\",\"être\"])\n","nlp = spacy.load('fr_core_news_sm')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7TzcZAmG1nUk","colab_type":"code","colab":{}},"source":["corpus_question = [\"qu'\",\"c'est\",'comment', 'pourquoi', 'quoi', 'depuis', 'combien', 'quand', 'où', 'est ce que', 'lequel', 'laquelle', 'lesquels', 'quel', 'quelle', 'quels', 'quelles', \"qu'est ce que\", 'êtes', 'je', 'tu', 'il', 'elle', 'nous', 'vous', 'ils']\n","\n","def mots_vides(phrase,sw=sw):\n","    liste_mots = []\n","    for mot in phrase.split():\n","        if mot not in sw :\n","            liste_mots.append(mot)\n","    return \" \".join(liste_mots)\n","\n","def lemmatise_text(text):\n","    return \" \".join([token.lemma_ for token in nlp(text)])\n","\n","def mots_question(phrase,corpus=corpus_question):\n","    liste_mots = []\n","    for mot in phrase.split():\n","        if mot not in corpus :\n","            liste_mots.append(mot)\n","    return \" \".join(liste_mots)\n","  \n","def nettoyage(question):\n","    q = question.lower()\n","    q = re.sub(r'[^\\w\\s]',' ',q)\n","    q = re.sub(r'\\d+',' ',q)\n","    q = mots_vides(q)\n","    q = mots_question(q)\n","    q = lemmatise_text(q)\n","    return  q"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O-Y36Wxd1rV7","colab_type":"code","colab":{}},"source":["def choix_question(theme, user_question):\n","  theme_reponses = scrapping[scrapping.theme==theme].reponse\n","  doc1 = nlp(nettoyage(user_question))\n","  list_doc1 = []\n","  list_doc2 = []\n","  list_similarity = []\n","  for reponse in theme_reponses:\n","      doc2 = nlp(nettoyage(reponse))\n","      list_doc1.append(doc1)\n","      list_doc2.append(doc2)\n","      list_similarity.append(doc1.similarity(doc2))\n","  result = pd.DataFrame({'doc1' : list_doc1, 'doc2' : list_doc2, 'similarité' : list_similarity, \"reponse\":theme_reponses}).sort_values('similarité', ascending=False)\n","  idx = result.similarité.idxmax(axis = 0)\n","  return result.reponse[idx]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D7c418w4yiO8","colab_type":"code","colab":{}},"source":["def chatbot():\n","  question = str(input(\"Bonjour, je suis le chatbot :)\\n\"))\n","  while question != \"au revoir\":\n","    question_vect  = vectorizer_bin.transform([question])\n","    result = model_bin.predict(question_vect.toarray())\n","\n","    #question métier : extraire une réponse de la base\n","    if result > 0.5 :\n","      #Chercher le thème de la question \n","      question_vectmulti = vectorizer_multi.transform([question])\n","      theme = classifier_multilog.predict(question_vectmulti)\n","      print(choix_question(theme[0],question),'\\n')\n","      question = str(input())\n","\n","    #text génération d'une réponse originale\n","    else:\n","      gen = generate_text(model, start_string=question)\n","      rep = re.split(r'[.?!]',gen)[1]\n","      print(rep.strip(),'\\n')\n","      question = str(input())\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JP0WHQU27_hd","colab_type":"code","colab":{}},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SyWwEqfjzoK_","colab_type":"code","outputId":"334f2f6b-052d-46a0-ceca-a6177a1bfae4","executionInfo":{"status":"ok","timestamp":1580663804866,"user_tz":-60,"elapsed":67249,"user":{"displayName":"sarah faucon","photoUrl":"","userId":"00071383878885981401"}},"colab":{"base_uri":"https://localhost:8080/","height":280}},"source":["chatbot()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Bonjour, je suis le chatbot :)\n","Bonjour\n","Dis-le-Fräuls, le soleil opre nuit pour ux \n","\n","Comment ca va ? \n","Le choue soit loin \n","\n","Comment se faire livrer ? \n","Lors de la réception de votre commande, vous devez vérifier l'état de vos meubles en présence des livreurs. Toute anomalie constatée doit être spécifiée sur le bon de livraison. Pas d'inquiétude cependant, vous disposez d'un délai supplémentaire de 72h pour nous signaler toute avarie sur votre livraison. Pour effectuer votre demande : Si vous n'avez pas de compte client ou si vous avez effectué une commande en magasin et qu'elle n'est pas rattachée à votre compte, contactez le Service Client par ici.  \n","\n","au revoir \n","Vous fous tuois \n","\n","au revoir\n"],"name":"stdout"}]}]}